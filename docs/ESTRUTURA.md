[‚¨ÖÔ∏è Voltar ao README Principal](../README.md)



# Estrutura do Projeto

Este documento descreve a organiza√ß√£o de diret√≥rios e arquivos do **Fake News Detector - Backend**.

## Vis√£o Geral

```
backend/
‚îú‚îÄ‚îÄ app/                    # C√≥digo principal da aplica√ß√£o
‚îÇ   ‚îú‚îÄ‚îÄ ai/                 # L√≥gica de AI e fact-checking
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ context/        # Apify e enrichment de contexto
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ factchecking/   # Evidence retrieval e verifica√ß√£o
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ pipeline/       # Claim extraction e judgment
|   |   ‚îî‚îÄ‚îÄ threads/        # Threadpool and concurrent job queue system
‚îÇ   ‚îú‚îÄ‚îÄ api/                # Endpoints FastAPI
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ endpoints/      # Defini√ß√£o de rotas da API
‚îÇ   ‚îú‚îÄ‚îÄ core/               # Configura√ß√£o central
‚îÇ   ‚îî‚îÄ‚îÄ models/             # Schemas Pydantic
‚îú‚îÄ‚îÄ scripts/                # Scripts de automa√ß√£o
‚îú‚îÄ‚îÄ logs/                   # Arquivos de log persistidos
‚îú‚îÄ‚îÄ docker-compose.yml      # Configura√ß√£o Docker
‚îú‚îÄ‚îÄ Dockerfile              # Imagem Docker
‚îú‚îÄ‚îÄ requirements.txt        # Depend√™ncias Python
‚îî‚îÄ‚îÄ README.md               # Documenta√ß√£o principal
```



## Detalhamento dos Diret√≥rios

### üìÅ `app/`
Diret√≥rio principal contendo todo o c√≥digo da aplica√ß√£o.

#### `app/ai/` - Intelig√™ncia Artificial
M√≥dulos relacionados ao processamento de IA e verifica√ß√£o de fatos:

- **`context/`** - Coleta e enriquecimento de contexto
  - Integra√ß√£o com Apify para scraping
  - Enrichment de informa√ß√µes de fontes externas
  
- **`factchecking/`** - Verifica√ß√£o de Fatos
  - Evidence retrieval (busca de evid√™ncias)
  - An√°lise e valida√ß√£o de fontes
  
- **`pipeline/`** - Pipeline de Processamento
  - Claim extraction (extra√ß√£o de afirma√ß√µes)
  - Judgment e classifica√ß√£o de veracidade

#### `app/ai/threads` - Sistema de processamento concorrente utilizando a Threadpool e filas do python

Como grande parte das opera√ß√µes da pipeline s√£o I/O heavy e bloqueam a execu√ß√£o de uma thread (que espera que esse I/O termine) n√≥s implementamos um sistema para execu√ß√£o concorrente de jobs da pipeline. Para isso utilizamos a Threadpool do python 
para alocar uma buffer de threads para executar jobs e utilizamos uma fila de jobs separados pelo tipo da opera√ß√£o (equivale √† cada passo do pipeline). Isso permite que um parte do c√≥digo submeta jobs como extra√ß√£o de links, busca por evid√™ncia... de forma async, e quando esses jobs precisarem ser processados, um outro trecho de c√≥digo espera
pela execu√ß√£o deles por meio da fila concorrente que implementamos.

#### `app/api/` - API REST
Implementa√ß√£o dos endpoints da API usando FastAPI:

- **`endpoints/`** - Defini√ß√µes de rotas
  - Endpoints para receber mensagens
  - Endpoints de an√°lise e verifica√ß√£o
  - Endpoints de consulta e relat√≥rios

#### `app/core/` - Configura√ß√£o
Configura√ß√µes centrais da aplica√ß√£o:
- Gerenciamento de vari√°veis de ambiente
- Configura√ß√µes de logging
- Constantes e par√¢metros do sistema

#### `app/models/` - Modelos de Dados
Schemas Pydantic para valida√ß√£o de dados:
- Modelos de request/response
- Estruturas de dados internas da pipeline (entradas e sa√≠das de cada passo da pipeline)
- Validadores



### üìÅ `scripts/`
Scripts de automa√ß√£o para facilitar o desenvolvimento e opera√ß√£o:

- `docker-start.sh` - Inicia o ambiente Docker
- `docker-stop.sh` - Para o ambiente Docker
- Outros scripts auxiliares



### üìÅ `logs/`
Diret√≥rio para armazenamento de logs da aplica√ß√£o:
- Logs s√£o persistidos em arquivos
- √ötil para debugging e monitoramento
- Ignorado pelo Git (configurado em `.gitignore`)



## Arquivos de Configura√ß√£o

### `docker-compose.yml`
Define os servi√ßos Docker e suas configura√ß√µes:
- Configura√ß√£o de containers
- Mapeamento de portas
- Volumes e redes

### `Dockerfile`
Instru√ß√µes para constru√ß√£o da imagem Docker:
- Imagem base Python
- Instala√ß√£o de depend√™ncias
- Configura√ß√£o do ambiente

### `requirements.txt`
Lista de todas as depend√™ncias Python do projeto:
- FastAPI para a API REST
- OpenAI para processamento de linguagem
- Bibliotecas de scraping e an√°lise
- Ferramentas de teste e desenvolvimento

### `.env` (n√£o versionado)
Arquivo de vari√°veis de ambiente (criado a partir de `env.example`):
- Chaves de API
- Configura√ß√µes espec√≠ficas do ambiente
- **Nunca deve ser commitado no Git**



## Fluxo de Dados

```mermaid
graph LR
    A[Usu√°rio] -->|Mensagem WhatsApp| B[API Endpoints]
    B --> C[Pipeline AI]
    C --> D[Claim Extraction]
    D --> E[Context Enrichment]
    E --> F[Fact Checking]
    F --> G[Judgment]
    G -->|Resultado| A
```

O sistema processa mensagens atrav√©s de um pipeline que:
1. Recebe o conte√∫do via API
2. Extrai claims principais
3. Busca contexto e evid√™ncias
4. Realiza fact-checking
5. Gera um julgamento final
6. Retorna o resultado ao usu√°rio

## APIs utilizadas

### API de Fact-checking do google

Utilizamos a API de fact-checking do google para buscar fontes confi√°veis sobre a afirma√ß√£o. 
Documenta√ß√£o da API pode ser acessada neste [link](https://developers.google.com/fact-check/tools/api/reference/rest/v1alpha1/claims)

E o mesmo recurso de fact-checking pode ser acessado de forma manual neste [website](https://toolbox.google.com/factcheck/explorer/search/list:recent;hl=pt) da google

### Busca na web em dom√≠nios confi√°veis

Utilizamos da API de busca customizada da google, restringindo os dom√≠nios retornados para uma lista de dom√≠nios confi√°veis (ex: g1.globo.com e .gov.br)

## üìö Documenta√ß√£o Relacionada

- [üìã Requisitos](./REQUISITOS.md) - Requisitos do sistema
- [‚öôÔ∏è Configura√ß√£o](./CONFIGURACAO.md) - Configure o ambiente
- [üõ†Ô∏è Execu√ß√£o](./EXECUCAO.md) - Como executar o backend
