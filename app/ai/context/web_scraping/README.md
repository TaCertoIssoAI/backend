# ğŸ•·ï¸ Web Scraping Module

MÃ³dulo Python para fazer scraping de pÃ¡ginas web com fallback automÃ¡tico para Selenium.

## âœ¨ Features

- âš¡ **RÃ¡pido**: Usa `requests` por padrÃ£o
- ğŸ”„ **Fallback inteligente**: Muda automaticamente para Selenium quando necessÃ¡rio
- ğŸ³ **Docker ready**: Suporta Selenium em containers
- ğŸ“¦ **Plug and play**: Basta importar e usar

## ğŸš€ Como Usar no Seu Projeto

### 1. Copiar para seu projeto

```bash
# Na raiz do seu projeto
cp -r web_scraping app/ai/context/
```

### 2. Instalar dependÃªncias

```bash
# Adicione ao seu requirements.txt principal:
cat web_scraping/requirements.txt >> requirements.txt

# Ou instale diretamente:
pip install -r web_scraping/requirements.txt
```

### 3. Configurar Docker (opcional, apenas se precisar de Selenium)

```bash
# Copie docker-compose.yml para a raiz do seu projeto
cp docker-compose.yml ../

# Inicie o container Selenium
docker compose up -d
```

### 4. Usar no cÃ³digo

```python
from app.ai.context.web_scraping import get_page_content

# Simples assim!
text = get_page_content("https://example.com")
print(text)

# ForÃ§ar Selenium (para sites JavaScript pesados)
text = get_page_content("https://facebook.com/some-post", force_selenium=True)
```

## ğŸ“– Exemplos

### Exemplo BÃ¡sico

```python
from app.ai.context.web_scraping import get_page_content

try:
    content = get_page_content("https://example.com")
    print(f"ExtraÃ­do {len(content)} caracteres")
    print(content[:200])  # Primeiros 200 chars
except RuntimeError as e:
    print(f"Erro ao fazer scraping: {e}")
```

### Exemplo com Tratamento de Erro

```python
from app.ai.context.web_scraping import get_page_content
import logging

logging.basicConfig(level=logging.INFO)

def scrape_safely(url: str) -> str | None:
    """Faz scraping com tratamento de erro"""
    try:
        return get_page_content(url)
    except RuntimeError as e:
        logging.error(f"Falha ao scraping {url}: {e}")
        return None

# Usar
content = scrape_safely("https://example.com")
if content:
    print("âœ… Sucesso!")
```

### Exemplo com MÃºltiplas URLs

```python
from app.ai.context.web_scraping import get_page_content
from concurrent.futures import ThreadPoolExecutor

urls = [
    "https://example.com",
    "https://wikipedia.org",
    "https://httpbin.org/html"
]

def scrape_url(url):
    try:
        return get_page_content(url)
    except:
        return None

# Scraping paralelo
with ThreadPoolExecutor(max_workers=3) as executor:
    results = list(executor.map(scrape_url, urls))

for url, content in zip(urls, results):
    if content:
        print(f"âœ… {url}: {len(content)} chars")
```

## âš™ï¸ ConfiguraÃ§Ã£o

### VariÃ¡veis de Ambiente

Crie um arquivo `.env` na raiz do seu projeto:

```bash
# Usar Selenium remoto (Docker)?
# Valores: auto | true | false
USE_SELENIUM_REMOTE=auto

# URL do Selenium Grid (se usar Docker)
SELENIUM_REMOTE_URL=http://localhost:4444/wd/hub
```

### OpÃ§Ãµes:

- **`USE_SELENIUM_REMOTE=auto`** (padrÃ£o): Detecta automaticamente
  - Tenta Docker primeiro
  - Se falhar, usa ChromeDriver local

- **`USE_SELENIUM_REMOTE=true`**: ForÃ§a uso do Docker
  - Requer `docker compose up -d` rodando
  - Mais estÃ¡vel e isolado

- **`USE_SELENIUM_REMOTE=false`**: ForÃ§a ChromeDriver local
  - Requer ChromeDriver instalado no sistema
  - Mais leve, sem Docker

## ğŸ³ Docker (Opcional)

### Se vocÃª precisa de Selenium:

1. **Copie `docker-compose.yml` para a raiz do projeto**:
   ```bash
   cp docker-compose.yml ../
   ```

2. **Inicie o container**:
   ```bash
   docker compose up -d
   ```

3. **Verifique**:
   ```bash
   docker compose ps
   curl http://localhost:4444/wd/hub/status
   ```

4. **Use normalmente**:
   ```python
   from app.ai.context.web_scraping import get_page_content
   
   # Vai usar o Selenium do Docker automaticamente!
   text = get_page_content("https://facebook.com/post")
   ```

### Quando NÃƒO precisa de Docker:

Se vocÃª sÃ³ vai fazer scraping de sites simples (HTML), **nÃ£o precisa do Docker**:

```python
# Sites HTML simples funcionam sem Selenium
text = get_page_content("https://example.com")  # âœ… Funciona
text = get_page_content("https://wikipedia.org")  # âœ… Funciona
```

## ğŸ”§ Como Funciona

1. **Tenta `requests`** primeiro (rÃ¡pido, leve)
2. Se falhar, **tenta Selenium** (sites JavaScript)
3. Selenium detecta automaticamente Docker ou local

## ğŸ“¦ Estrutura

```
web_scraping/
â”œâ”€â”€ __init__.py          # ExpÃµe get_page_content()
â”œâ”€â”€ scraper.py           # LÃ³gica principal
â”œâ”€â”€ requirements.txt     # DependÃªncias
â””â”€â”€ README.md           # Esta documentaÃ§Ã£o
```

## ğŸš¨ Troubleshooting

### Erro: "selenium nÃ£o estÃ¡ instalado"

```bash
pip install selenium
```

### Erro: "erro do selenium"

**OpÃ§Ã£o 1**: Use Docker (recomendado)
```bash
docker compose up -d
```

**OpÃ§Ã£o 2**: Instale ChromeDriver local
```bash
# Ubuntu/Debian
sudo apt install chromium-chromedriver

# Mac
brew install chromedriver

# Ou baixe de: https://chromedriver.chromium.org/
```

### Sites nÃ£o funcionam mesmo com Selenium

Alguns sites bloqueiam scraping. SoluÃ§Ãµes:
- Use proxies rotativos
- Configure cookies/sessÃµes
- Considere APIs oficiais se disponÃ­veis

## ğŸ“ Notas

- **Limite de caracteres**: 128.000 por padrÃ£o (configurÃ¡vel no cÃ³digo)
- **Timeout**: 20s para requests, 30s para Selenium
- **User-Agent**: Aleatorizado automaticamente
- **Suporta**: HTML, JSON, text/plain

## ğŸ”— IntegraÃ§Ã£o Completa

### Estrutura no seu projeto:

```
seu-projeto/
â”œâ”€â”€ app/
â”‚   â””â”€â”€ ai/
â”‚       â””â”€â”€ context/
â”‚           â””â”€â”€ web_scraping/    â† Pasta copiada
â”‚               â”œâ”€â”€ __init__.py
â”‚               â”œâ”€â”€ scraper.py
â”‚               â””â”€â”€ requirements.txt
â”œâ”€â”€ docker-compose.yml           â† Copiado da pasta web_scraping
â”œâ”€â”€ .env                         â† Suas configuraÃ§Ãµes
â”œâ”€â”€ requirements.txt             â† Inclui dependÃªncias do web_scraping
â””â”€â”€ main.py                      â† Seu cÃ³digo que usa o mÃ³dulo
```

### Exemplo de uso no seu projeto:

```python
# app/ai/context/main.py
from app.ai.context.web_scraping import get_page_content

def process_url(url: str):
    """Processa conteÃºdo de uma URL"""
    print(f"Buscando {url}...")
    
    content = get_page_content(url)
    
    # Agora vocÃª tem o texto limpo!
    print(f"ExtraÃ­dos {len(content)} caracteres")
    
    # FaÃ§a o que quiser com o conteÃºdo
    # - Envie para LLM
    # - Salve no banco
    # - Processe com IA
    return content

if __name__ == "__main__":
    text = process_url("https://example.com")
    print(text[:500])
```

## âœ… Checklist de IntegraÃ§Ã£o

- [ ] Copiar pasta `web_scraping` para `app/ai/context/`
- [ ] Adicionar dependÃªncias ao `requirements.txt` principal
- [ ] Instalar: `pip install -r requirements.txt`
- [ ] (Opcional) Copiar `docker-compose.yml` para raiz
- [ ] (Opcional) Criar `.env` com configuraÃ§Ãµes
- [ ] Importar e usar: `from app.ai.context.web_scraping import get_page_content`
- [ ] Testar com URL simples
- [ ] (Opcional) Iniciar Docker: `docker compose up -d`
- [ ] Testar com URL JavaScript

## ğŸ‰ Pronto!

Agora vocÃª tem scraping inteligente no seu projeto!

```python
from app.ai.context.web_scraping import get_page_content

# Ã‰ sÃ³ isso! ğŸš€
text = get_page_content("https://qualquer-site.com")
```
